{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVrFFjxpPu5L"
      },
      "source": [
        "Sheet 2.4: Character-level sequence modeling w/ RNNs\n",
        "====================================================\n",
        "\n",
        "**Author:** Michael Franke\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPFze9mgPu5O"
      },
      "source": [
        "The goal of this tutorial is to get familiar with simple language models.\n",
        "To be able to have a manageable (quick to train, evaluate, inspect) case study, we look at character-level predictions of surnames from different languages.\n",
        "(The inspiration and source of this notebook is [this tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) from PyTorch&rsquo;s documentation.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20cZqLiAPu5P"
      },
      "source": [
        "## Packages & global parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBwn_KWuPu5P"
      },
      "source": [
        "In addition to the usual packages for neural network modeling, we will also require packages for I/O and string handling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i1q3iXwYPu5P"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## import packages\n",
        "##################################################\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import json\n",
        "import pandas\n",
        "import string\n",
        "import torch\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jops-MMIPu5Q"
      },
      "source": [
        "## Loading & inspecting the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQVZe733Pu5R"
      },
      "source": [
        "Our training data are lists of surnames from different countries.\n",
        "We will use this data set to train a model that predicts a name, given the country as a prompt.\n",
        "\n",
        "The (pre-processed) data is stored in a JSON file.\n",
        "We load it and define a few useful variables for later use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3OQqKDxTPu5R",
        "outputId": "9e0533e0-e662-4417-9b3a-893058eefd53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n",
            "18\n",
            "Czech has  519  names\n",
            "Unique name count: 502\n",
            "  Blazek: 2\n",
            "  Cermak: 2\n",
            "  Cerny: 2\n",
            "  Hanek: 2\n",
            "  Kafka: 2\n",
            "  Kocian: 2\n",
            "  Kouba: 2\n",
            "  Marik: 2\n",
            "  Navratil: 2\n",
            "  Ponec: 2\n",
            "German has  724  names\n",
            "Unique name count: 690\n",
            "  Sommer: 4\n",
            "  Kruger: 3\n",
            "  Armbruster: 2\n",
            "  Baumgartner: 2\n",
            "  Bosch: 2\n",
            "  Ebner: 2\n",
            "  Groel: 2\n",
            "  Gunther: 2\n",
            "  Holzer: 2\n",
            "  Horn: 2\n",
            "Arabic has  2000  names\n",
            "Unique name count: 108\n",
            "  Tahan: 28\n",
            "  Fakhoury: 28\n",
            "  Nader: 27\n",
            "  Koury: 27\n",
            "  Mustafa: 26\n",
            "  Antar: 26\n",
            "  Sarraf: 26\n",
            "  Hadad: 26\n",
            "  Kassis: 26\n",
            "  Shadid: 25\n",
            "Japanese has  991  names\n",
            "Unique name count: 990\n",
            "  Okuma: 2\n",
            "  Abe: 1\n",
            "  Abukara: 1\n",
            "  Adachi: 1\n",
            "  Aida: 1\n",
            "  Aihara: 1\n",
            "  Aizawa: 1\n",
            "  Ajibana: 1\n",
            "  Akaike: 1\n",
            "  Akamatsu: 1\n",
            "Chinese has  268  names\n",
            "Unique name count: 246\n",
            "  Chu: 2\n",
            "  Fei: 2\n",
            "  Feng: 2\n",
            "  Guan: 2\n",
            "  Guo: 2\n",
            "  Hong: 2\n",
            "  Huan: 2\n",
            "  Jin: 2\n",
            "  Luo: 2\n",
            "  Qiu: 2\n",
            "Vietnamese has  73  names\n",
            "Unique name count: 71\n",
            "  Chu: 2\n",
            "  Doan: 2\n",
            "  Nguyen: 1\n",
            "  Tron: 1\n",
            "  Le: 1\n",
            "  Pham: 1\n",
            "  Huynh: 1\n",
            "  Hoang: 1\n",
            "  Phan: 1\n",
            "  Vu: 1\n",
            "Russian has  9408  names\n",
            "Unique name count: 9341\n",
            "  To The First Page: 24\n",
            "  Researcher: 9\n",
            "  Bass: 3\n",
            "  Baikin: 2\n",
            "  Chabanov: 2\n",
            "  Chabrov: 2\n",
            "  Chadin: 2\n",
            "  Chadov: 2\n",
            "  Chadovich: 2\n",
            "  Chadrantsev: 2\n",
            "French has  277  names\n",
            "Unique name count: 273\n",
            "  Lyon: 2\n",
            "  Martel: 2\n",
            "  Masson: 2\n",
            "  Paquet: 2\n",
            "  Abel: 1\n",
            "  Abraham: 1\n",
            "  Adam: 1\n",
            "  Albert: 1\n",
            "  Allard: 1\n",
            "  Archambault: 1\n",
            "Irish has  232  names\n",
            "Unique name count: 226\n",
            "  Eoin: 2\n",
            "  O'Boyle: 2\n",
            "  O'Brien: 2\n",
            "  O'Donnell: 2\n",
            "  O'Keefe: 2\n",
            "  O'Neal: 2\n",
            "  Adam: 1\n",
            "  Ahearn: 1\n",
            "  Aodh: 1\n",
            "  Aodha: 1\n",
            "English has  3668  names\n",
            "Unique name count: 3668\n",
            "  Abbas: 1\n",
            "  Abbey: 1\n",
            "  Abbott: 1\n",
            "  Abdi: 1\n",
            "  Abel: 1\n",
            "  Abraham: 1\n",
            "  Abrahams: 1\n",
            "  Abrams: 1\n",
            "  Ackary: 1\n",
            "  Ackroyd: 1\n",
            "Spanish has  298  names\n",
            "Unique name count: 293\n",
            "  Gutierrez: 2\n",
            "  Martinez: 2\n",
            "  Perez: 2\n",
            "  Roma: 2\n",
            "  Vela: 2\n",
            "  Abana: 1\n",
            "  Abano: 1\n",
            "  Abarca: 1\n",
            "  Abaroa: 1\n",
            "  Abascal: 1\n",
            "Greek has  203  names\n",
            "Unique name count: 193\n",
            "  Close: 8\n",
            "  Antonopoulos: 3\n",
            "  Chrysanthopoulos: 2\n",
            "  Adamidis: 1\n",
            "  Adamou: 1\n",
            "  Agelakos: 1\n",
            "  Akrivopoulos: 1\n",
            "  Alexandropoulos: 1\n",
            "  Anetakis: 1\n",
            "  Angelopoulos: 1\n",
            "Italian has  709  names\n",
            "Unique name count: 701\n",
            "  Capello: 3\n",
            "  Como: 2\n",
            "  Fiscella: 2\n",
            "  Roma: 2\n",
            "  Romano: 2\n",
            "  Sinagra: 2\n",
            "  Vinci: 2\n",
            "  Abandonato: 1\n",
            "  Abatangelo: 1\n",
            "  Abatantuono: 1\n",
            "Portuguese has  74  names\n",
            "Unique name count: 74\n",
            "  Abreu: 1\n",
            "  Albuquerque: 1\n",
            "  Almeida: 1\n",
            "  Alves: 1\n",
            "  Araujo: 1\n",
            "  Araullo: 1\n",
            "  Barros: 1\n",
            "  Basurto: 1\n",
            "  Belo: 1\n",
            "  Cabral: 1\n",
            "Scottish has  100  names\n",
            "Unique name count: 100\n",
            "  Smith: 1\n",
            "  Brown: 1\n",
            "  Wilson: 1\n",
            "  Campbell: 1\n",
            "  Stewart: 1\n",
            "  Thomson: 1\n",
            "  Robertson: 1\n",
            "  Anderson: 1\n",
            "  Macdonald: 1\n",
            "  Scott: 1\n",
            "Dutch has  297  names\n",
            "Unique name count: 286\n",
            "  Laar: 3\n",
            "  Andel: 2\n",
            "  Hout: 2\n",
            "  Leeuwenhoek: 2\n",
            "  Nifterick: 2\n",
            "  Nifterik: 2\n",
            "  Niftrik: 2\n",
            "  Oorschot: 2\n",
            "  Rompa: 2\n",
            "  Schoorel: 2\n",
            "Korean has  94  names\n",
            "Unique name count: 94\n",
            "  Ahn: 1\n",
            "  Baik: 1\n",
            "  Bang: 1\n",
            "  Byon: 1\n",
            "  Cha: 1\n",
            "  Chang: 1\n",
            "  Chi: 1\n",
            "  Chin: 1\n",
            "  Cho: 1\n",
            "  Choe: 1\n",
            "Polish has  139  names\n",
            "Unique name count: 138\n",
            "  Zdunowski: 2\n",
            "  Adamczak: 1\n",
            "  Adamczyk: 1\n",
            "  Andrysiak: 1\n",
            "  Auttenberg: 1\n",
            "  Bartosz: 1\n",
            "  Bernard: 1\n",
            "  Bobienski: 1\n",
            "  Bosko: 1\n",
            "  Broz: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##################################################\n",
        "## read and inspect the data\n",
        "##################################################\n",
        "with urllib.request.urlopen(\n",
        "    \"https://raw.githubusercontent.com/michael-franke/npNLG/main/neural_pragmatic_nlg/05-RNNs/names-data.json\"\n",
        ") as url:\n",
        "    names_data = json.load(url)\n",
        "\n",
        "# # local import\n",
        "# with open('names-data.json') as dataFile:\n",
        "#     names_data = json.load(dataFile)\n",
        "\n",
        "categories = list(names_data.keys())\n",
        "n_categories = len(categories)\n",
        "print(categories)\n",
        "print(n_categories)\n",
        "for country in names_data:\n",
        "    # Get all names for the country\n",
        "    names = names_data[country]\n",
        "    print(country, \"has \", len(names), \" names\")\n",
        "\n",
        "    from collections import Counter\n",
        "    name_counts = Counter(names)\n",
        "    print(f\"Unique name count:\", len(name_counts))\n",
        "\n",
        "    sorted_names = sorted(name_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    for name, count in sorted_names[:10]:\n",
        "        print(f\"  {name}: {count}\")\n",
        "\n",
        "\n",
        "# Use all ASCII letters as vocabulary (plus tokens [EOS], [SOS])\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 2  # all letter + [EOS] and [SOS]\n",
        "SOSIndex = n_letters - 1\n",
        "EOSIndex = n_letters - 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaOyAsfUPu5S"
      },
      "source": [
        "The data consists of two things:\n",
        "a list of strings, called &ldquo;categories&rdquo;, contains all the categories (languages) for which we have data;\n",
        "a dictionary, called &ldquo;names_data&rdquo;, contains a list of names for each category.\n",
        "\n",
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.1: Inspect the data</span></strong>\n",
        ">\n",
        "> 0. [Just for yourself.] Find out what&rsquo;s in the data set. How any different countries do we have? How many names per country? Are all names unique in a given country? Do the names sound typical to your ears for the given countries?\n",
        "18 countries, all of the names they have are *not* unique.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HYmf9D_Pu5S"
      },
      "source": [
        "Click below to see the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "xomIJGsZPu5S",
        "outputId": "733ad72e-f907-46a8-f4df-8c9be620ab65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "Number of countries: 18\n",
            "\n",
            "Czech: 519\n",
            "German: 724\n",
            "Arabic: 2000\n",
            "Japanese: 991\n",
            "Chinese: 268\n",
            "Vietnamese: 73\n",
            "Russian: 9408\n",
            "French: 277\n",
            "Irish: 232\n",
            "English: 3668\n",
            "Spanish: 298\n",
            "Greek: 203\n",
            "Italian: 709\n",
            "Portuguese: 74\n",
            "Scottish: 100\n",
            "Dutch: 297\n",
            "Korean: 94\n",
            "Polish: 139\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n",
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "for c in names_data:\n",
        "    for name in names_data[c]:\n",
        "        if name == \"Bruckner\":\n",
        "            print(\"yes\")\n",
        "\n",
        "names_data\n",
        "\n",
        "# number of countries\n",
        "print(f\"Number of countries: {n_categories}\\n\")\n",
        "# number of names per country\n",
        "for country in names_data:\n",
        "    print(f\"{country}: {len(names_data[country])}\")\n",
        "\n",
        "# Unique names per country\n",
        "for country in names_data:\n",
        "    print(f\"{len(names_data[country]) == len(set(names_data[country]))}\")\n",
        "\n",
        "# ---> Most of the name lists contain duplicates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4YBoVIlPu5T"
      },
      "source": [
        "## Train-test split\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfCXNknOPu5T"
      },
      "source": [
        "We will split the data into a training and a test set.\n",
        "Look at the code and try to answer the exercise question of how this split is realized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_bnwDSwzPu5T",
        "outputId": "1134a924-b3aa-4dfb-d107-dccc059a303a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Czech 519 467 52\n",
            "German 724 652 72\n",
            "Arabic 2000 1800 200\n",
            "Japanese 991 892 99\n",
            "Chinese 268 241 27\n",
            "Vietnamese 73 66 7\n",
            "Russian 9408 8467 941\n",
            "French 277 249 28\n",
            "Irish 232 209 23\n",
            "English 3668 3301 367\n",
            "Spanish 298 268 30\n",
            "Greek 203 183 20\n",
            "Italian 709 638 71\n",
            "Portuguese 74 67 7\n",
            "Scottish 100 90 10\n",
            "Dutch 297 267 30\n",
            "Korean 94 85 9\n",
            "Polish 139 125 14\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## make a train/test split\n",
        "##################################################\n",
        "\n",
        "train_data = dict()\n",
        "test_data = dict()\n",
        "split_percentage = 10\n",
        "for k in list(names_data.keys()):\n",
        "    total_size = len(names_data[k])\n",
        "    test_size = round(total_size / split_percentage)\n",
        "    train_size = total_size - test_size\n",
        "    print(k, total_size, train_size, test_size)\n",
        "    indices = [i for i in range(total_size)]\n",
        "    random.shuffle(indices)\n",
        "    train_indices = indices[0:train_size]\n",
        "    test_indices = indices[(train_size + 1) : (-1)]\n",
        "    train_data[k] = [names_data[k][i] for i in train_indices]\n",
        "    test_data[k] = [names_data[k][i] for i in test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyhKD5uvPu5U"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.2: Explain the train-test split</span></strong>\n",
        ">\n",
        "> 1. How is the original data information split into training and test set? (E.g., what amount of data is allocated to each part?; is the split exclusive and exhaustive?; how is it determined which item goes where?)\n",
        "\n",
        "> %10 of each country's data is allocated to test(rounded up) and the rest is to the train. The code randomly shuffles the indices and then splits them into non-overlapping parts. So it is exclusive and since no datapoint is left out, it is also exhaustive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayaT_1kUPu5U"
      },
      "source": [
        "Click below to see the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFyRoqztPu5U"
      },
      "source": [
        "```{toggle}\n",
        "The dataset is split in training- and testset, where 90% are used for training and 10% for testing. The datapoints are shuffled before assignment and therefore randomly assigned. The split is both exclusive and exhaustive.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7azNuEpxPu5U"
      },
      "source": [
        "## Defining the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngpslGtPu5V"
      },
      "source": [
        "The model we use is a (hand-crafted) recurrent neural network.\n",
        "The architecture follows [this tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html), from where we also borrow the following picture:\n",
        "\n",
        "![img](https://github.com/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/tutorials/pics/05-RNN-diagram.png?raw=1){width=550px}\n",
        "\n",
        "The model makes consecutive predictions about the next character.\n",
        "It is conditioned on three vectors:\n",
        "\n",
        "1.  &rsquo;category&rsquo; is a one-hot vector encoding the country\n",
        "2.  &rsquo;input&rsquo; is a one-hot vector encoding the character\n",
        "3.  &rsquo;hidden&rsquo; is the RNN&rsquo;s hidden state (remembering what happened before)\n",
        "\n",
        "These vectors are first combined and then used to produce a next-character probability distribution *and* the hidden state to be fed into the next round of predictions.\n",
        "\n",
        "Next to the usual functions (initialization and forward pass), there is also a function that returns a blank &rsquo;hidden state&rsquo;.\n",
        "This will be used later during training and inference, because at the start of each application (training or inference) the RNN should have a blank memory.\n",
        "(It makes sense to include this function in the definition of the module because it depends on the module&rsquo;s parameters (size of the hidden layer).)\n",
        "\n",
        "Notice that the architecture features a dropout layer, which randomly sets a fixed proportion of units to 0.\n",
        "The inclusion of dropout introduces a random element in the model during training and inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "22rxFGbTPu5V"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## define RNN\n",
        "##################################################\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden, dropout):\n",
        "        input_combined = torch.cat((category, input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1)\n",
        "        output_combined = self.relu(output_combined)\n",
        "        output = self.o2o(output_combined)\n",
        "        if dropout:\n",
        "            output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heYqUhk5Pu5V"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.3: Inspect the model</span></strong>\n",
        ">\n",
        "> 0. [Just for yourself.] Make sure that you understand the model architecture and its implementation. E.g., do you agree that this code implements the model graph shown above? Can you think of slight alterations to the model which might also work?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stw_tw1QPu5V"
      },
      "source": [
        "Click below to see the solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHn0_qltPu5V"
      },
      "source": [
        "```{toggle}\n",
        "The model matches the graph in the picture. Not sure what the expected alternation here is. It could be a good idea to limit the dropout layer to training and deactivate for inference. We would have to add an additional parameter to the forward pass as shown above.\n",
        "\n",
        "Another idea would be to add a non-linear activation function between the combined output layer and the final output layer. This seems to improve training quiet a bit.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNrRvbvMPu5V"
      },
      "source": [
        "## Helper functions for training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtJjyIa7Pu5V"
      },
      "source": [
        "For training, we will present the model with randomly sampled single items.\n",
        "This is why we define a &rsquo;random_training_pair&rsquo; function which returns, well, a random training pair (category and name).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TVBq-8M6Pu5V"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## helper functions for training\n",
        "##################################################\n",
        "\n",
        "\n",
        "# Random item from a list\n",
        "def random_choice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "\n",
        "# Get a random category and random line from that category\n",
        "def random_training_pair():\n",
        "    category = random_choice(categories)\n",
        "    line = random_choice(train_data[category])\n",
        "    return category, line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-UEQGcEPu5V"
      },
      "source": [
        "We also need to make sure that the training and test data are in a format that the model understands.\n",
        "So, this is where we use vector representations for the categories and sequences of characters.\n",
        "For sequences of characters we distinguish those used as input to the model (&rsquo;input_tensor&rsquo;) and those used in training as what needs to be predicted (&rsquo;target_tensor&rsquo;).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LMok7l4JPu5W"
      },
      "outputs": [],
      "source": [
        "# One-hot vector for category\n",
        "def category_tensor(category):\n",
        "    li = categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# One-hot matrix of first to last letters (not including [EOS]) for input\n",
        "# The first input is always [SOS]\n",
        "def input_tensor(line):\n",
        "    tensor = torch.zeros(len(line) + 1, 1, n_letters)\n",
        "    tensor[0][0][SOSIndex] = 1\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li + 1][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def target_tensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(len(line))]\n",
        "    letter_indexes.append(EOSIndex)\n",
        "    return torch.LongTensor(letter_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyq3wFq3Pu5W"
      },
      "source": [
        "Finally, we construct a function that returns a random training pair in the proper vectorized format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z3-zUeR0Pu5W"
      },
      "outputs": [],
      "source": [
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def random_training_example():\n",
        "    \"\"\"\n",
        "    select a random training pair (category, name).\n",
        "    return:\n",
        "    - category_tensor: Tensor of shape (1, n_categories), one-hot encoded category.\n",
        "    - input_tensor: Tensor of shape (line_length + 1, 1, n_letters), one-hot encoded sequence of characters with SOS at start.\n",
        "    - target_tensor: Tensor of shape (line_length + 1), containing indices of the next characters, ending with EOS.\n",
        "\n",
        "    This format is suitable for training an RNN to predict the next character in a sequence given a language category.\n",
        "    \"\"\"\n",
        "    category, line = random_training_pair()\n",
        "    category_tensor_ = category_tensor(category)\n",
        "    input_line_tensor = input_tensor(line)\n",
        "    target_line_tensor = target_tensor(line)\n",
        "    return category_tensor_, input_line_tensor, target_line_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g589Tp8oPu5W"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.4: Understand the representational format </span></strong>\n",
        ">\n",
        "> 1. Write a doc-string for the function &rsquo;random_training_example&rsquo; that is short but completely explanatory regarding the format and meaning of its output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xACKkpcaPu5W"
      },
      "source": [
        "Click below to see the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "tUPyteXMPu5W"
      },
      "outputs": [],
      "source": [
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def random_training_example():\n",
        "    \"\"\"\n",
        "    create a random training example from dataset.\n",
        "    :return:\n",
        "        category_tensor_: A tensor of shape (1X18) with a one at the index of the current country and zeros elsewhere\n",
        "        input_line_tensor: A tensor of shape (length current name x 1 x 60). There is one tensor for each character of\n",
        "                           the name with a one at the index of the current character and zeros elsewhere.\n",
        "        target_line_tensor: A tensor of length equal to the number of characters in the name indicating the indices\n",
        "                            of the characters in the vocabulary. (gold label)\n",
        "    \"\"\"\n",
        "    category, line = random_training_pair()\n",
        "    category_tensor_ = category_tensor(category)\n",
        "    input_line_tensor = input_tensor(line)\n",
        "    target_line_tensor = target_tensor(line)\n",
        "    return category_tensor_, input_line_tensor, target_line_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-oR9s3hPu5X"
      },
      "source": [
        "We use this timing function to keep track of training time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JJlZPiJ5Pu5X"
      },
      "outputs": [],
      "source": [
        "def time_since(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return \"%dm %ds\" % (m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ephwKqPu5X"
      },
      "source": [
        "## Training the network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugp5-M6fPu5X"
      },
      "source": [
        "This function captures a single training step for one training triplet (category, input representation of the name, output representation of the string).\n",
        "\n",
        "What is important to note here is that at the start of each &ldquo;name&rdquo;, so to speak, we need to supply a fresh &rsquo;hidden layer&rsquo;, but that subsequent calls to the RNN&rsquo;s forward pass function will use the hidden layer that is returned from the previous forward pass.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "faVrKBiJPu5X"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## single training pass\n",
        "##################################################\n",
        "\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    # reshape target tensor\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    # get a fresh hidden layer\n",
        "    hidden = rnn.init_hidden()\n",
        "    # reset cumulative loss\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    # zero the gradients\n",
        "    # sequentially probe predictions and collect loss\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden, True)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "    # perform backward pass\n",
        "    loss.backward()\n",
        "    # perform optimization\n",
        "    optimizer.step()\n",
        "    # return prediction and loss\n",
        "    return loss.item()  # / input_line_tensor.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb0p3DjiPu5X"
      },
      "source": [
        "The actual training process is furthermore not very special.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d5j3N_8-Pu5c",
        "outputId": "72c6da82-b88d-4e2c-ec45-d3546ac33b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0m 28s (5000 5%) 20.0221\n",
            "0m 58s (10000 10%) 18.9906\n",
            "1m 27s (15000 15%) 18.3383\n",
            "1m 56s (20000 20%) 17.8402\n",
            "2m 24s (25000 25%) 17.4561\n",
            "2m 53s (30000 30%) 17.1463\n",
            "3m 22s (35000 35%) 16.8836\n",
            "3m 52s (40000 40%) 16.6709\n",
            "4m 21s (45000 45%) 16.4822\n",
            "4m 50s (50000 50%) 16.3105\n",
            "5m 19s (55000 55%) 16.1451\n",
            "5m 50s (60000 60%) 16.0005\n",
            "6m 20s (65000 65%) 15.8666\n",
            "6m 50s (70000 70%) 15.7468\n",
            "7m 19s (75000 75%) 15.6327\n",
            "7m 50s (80000 80%) 15.5167\n",
            "8m 19s (85000 85%) 15.4145\n",
            "8m 50s (90000 90%) 15.3184\n",
            "9m 20s (95000 95%) 15.2319\n",
            "9m 50s (100000 100%) 15.1461\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## actual training loop\n",
        "## (should take about 2-4 minutes)\n",
        "##################################################\n",
        "\n",
        "# instantiate the model\n",
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "# training objective\n",
        "criterion = nn.NLLLoss()\n",
        "# learning rate\n",
        "learning_rate = 0.0005\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "# training parameters\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0  # will be reset every 'plot_every' iterations\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    loss = train(*random_training_example())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        rolling_mean = np.mean(all_losses[iter - print_every * (iter // print_every) :])\n",
        "        print(\n",
        "            \"%s (%d %d%%) %.4f\"\n",
        "            % (time_since(start), iter, iter / n_iters * 100, rolling_mean)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTKU04uuPu5c"
      },
      "source": [
        "Here is a plot of the temporal development of the model&rsquo;s performance during training:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ppyQGtUuPu5c",
        "outputId": "84840da7-f020-46b8-a457-b61e30849f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "##################################################\n",
        "## monitoring loss function during training\n",
        "##################################################\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bzL9tOKPu5d"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.5: Investigate the training regime </span></strong>\n",
        ">\n",
        "> 1. What exactly is the loss function here? What are we training the model on: perplexity, average surprisal, or yet something else?\n",
        "> It is negative log-likelihood loss. It is average surprisal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMEmvbZ6Pu5d"
      },
      "source": [
        "Click below to see the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr69E02ZPu5d"
      },
      "source": [
        "```{toggle}\n",
        "We have the loss function defined as the negative log likelihood loss, which is basically average surprisal. Especially since nn.NLLLoss has a parameter \"reduction\" which is set to \"mean\" by default.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUPRwJ06Pu5d"
      },
      "source": [
        "## Evaluation & inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCH2L0VWPu5d"
      },
      "source": [
        "Let&rsquo;s see what the model has learned and how well it does in producing new names.\n",
        "\n",
        "Here are some auxiliary functions to obtain surprisal values and related notions for sequences of characters.\n",
        "We can use them to compare the model&rsquo;s performance on the training and test data set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jjpKQ3xnPu5d",
        "outputId": "d71a5e4b-710c-4f80-ca3b-df3cad95d69b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "mean surprisal (test): 15.44156122088758\n",
            "\n",
            "mean surprisal (train): 11.850367460338868\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## evaluation\n",
        "##################################################\n",
        "\n",
        "\n",
        "def get_surprisal_item(category, name):\n",
        "    category_tensor_ = category_tensor(category)\n",
        "    input_line_tensor = input_tensor(name)\n",
        "    target_line_tensor = target_tensor(name)\n",
        "    hidden = rnn.init_hidden()\n",
        "    surprisal = 0\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor_, input_line_tensor[i], hidden, False)\n",
        "        surprisal += criterion(output, target_line_tensor[i])\n",
        "    return surprisal.item()\n",
        "\n",
        "\n",
        "def get_surprisal_dataset(data):\n",
        "    surprisl_dict = dict()\n",
        "    surp_avg_dict = dict()\n",
        "    perplxty_dict = dict()\n",
        "    for category in list(data.keys()):\n",
        "        surprisl = 0\n",
        "        surp_avg = 0\n",
        "        perplxty = 0\n",
        "        # training\n",
        "        for name in data[category]:\n",
        "            item_surpr = get_surprisal_item(category, name)\n",
        "            surprisl += item_surpr\n",
        "            surp_avg += item_surpr / len(name)\n",
        "            perplxty += item_surpr ** (-1 / len(name))\n",
        "        n_items = len(data[category])\n",
        "\n",
        "        surprisl_dict[category] = surprisl / n_items\n",
        "        surp_avg_dict[category] = surp_avg / n_items\n",
        "        perplxty_dict[category] = perplxty / n_items\n",
        "\n",
        "    return (surprisl_dict, surp_avg_dict, perplxty_dict)\n",
        "\n",
        "\n",
        "def make_df(surp_dict):\n",
        "    p = pandas.DataFrame.from_dict(surp_dict)\n",
        "    p = p.transpose()\n",
        "    p.columns = [\"surprisal\", \"surp_scaled\", \"perplexity\"]\n",
        "    return p\n",
        "\n",
        "\n",
        "surprisal_test = make_df(get_surprisal_dataset(test_data))\n",
        "surprisal_train = make_df(get_surprisal_dataset(train_data))\n",
        "\n",
        "print(\"\\nmean surprisal (test):\", np.mean(surprisal_test[\"surprisal\"]))\n",
        "print(\"\\nmean surprisal (train):\", np.mean(surprisal_train[\"surprisal\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqeDAv-kPu5d"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.6: Interpret the evaluation metric </span></strong>\n",
        ">\n",
        "> 1. What do these two last numbers represent? What&rsquo;s better: a higher or lower value? What do the two numbers tell us when we compare them?\n",
        "> They are mean surprisal values, which are averages of losses. A low surprisal is better.Results indicate that the model performs better on the training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9GBPm3Pu5d"
      },
      "source": [
        "Click below to see the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6t-ibMEPu5d"
      },
      "source": [
        "```{toggle}\n",
        "These two values answer the question, how surprised the model is seeing the data. A higher value indicates more surprisal. We can see that the model is more surprised seeing the test set, which makes sense because it was trained on the train set while it has never seen the test set before.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrTJAkw0Pu5e"
      },
      "source": [
        "## Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3of2bXJPu5e"
      },
      "source": [
        "Let&rsquo;s also explore the trained model&rsquo;s predictions to compare them against our own intuitions of what might be typical names in a given country.\n",
        "\n",
        "Here&rsquo;s a function that takes a country and an initial string, and it outputs a model prediction for how to continue that string.\n",
        "\n",
        "Note that for each prediction step with the model, the model is embedded under `torch.no_grad()`. This setting means that gradients are not computed during the forward passes through the model. This setting is recommended for *inference* (for any neural net), i.e., when you don't want to train the model anymore (only use it for generating predictions). This allows to increase computational effiency and save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q8xx_IdCPu5e",
        "outputId": "0fd92633-8c90-473d-ff3c-0e69ff4c12ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mller\n",
            "Mller\n",
            "Mller\n",
            "Mller\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "## prediction function\n",
        "##################################################\n",
        "\n",
        "max_length = 20\n",
        "\n",
        "\n",
        "# make a prediction based on given sequence\n",
        "def predict(category, initial_sequence):\n",
        "\n",
        "    if len(initial_sequence) >= max_length:\n",
        "        return initial_sequence\n",
        "\n",
        "    category_tensor_ = category_tensor(category)\n",
        "    input_line_tensor = input_tensor(initial_sequence)\n",
        "    hidden = rnn.init_hidden()\n",
        "\n",
        "    name = initial_sequence\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = rnn(category_tensor_, input_line_tensor[i], hidden, False)\n",
        "\n",
        "    # greedy decoding: choosing the most likely guess\n",
        "    topv, topi = output.topk(1)\n",
        "    topi = topi[0][0]\n",
        "\n",
        "    if topi == EOSIndex:\n",
        "        return name\n",
        "    else:\n",
        "        name += all_letters[topi]\n",
        "\n",
        "    return predict(category, name)\n",
        "\n",
        "\n",
        "print(predict(\"German\", \"Mll\"))\n",
        "print(predict(\"German\", \"Mll\"))\n",
        "print(predict(\"German\", \"Mll\"))\n",
        "print(predict(\"German\", \"Mll\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3W8NRFLPu5e"
      },
      "source": [
        "You can also probe the model with an empty string:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xael15YSPu5e",
        "outputId": "8a293547-9ada-464f-8bf4-9dec4f05e517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kamata\n",
            "Kamata\n",
            "Kamata\n",
            "Kamata\n",
            "Bauner\n",
            "Grost\n"
          ]
        }
      ],
      "source": [
        "print(predict(\"Japanese\", \"\"))\n",
        "print(predict(\"Japanese\", \"\"))\n",
        "print(predict(\"Japanese\", \"\"))\n",
        "print(predict(\"Japanese\", \"\"))\n",
        "print(predict(\"German\", \"\"))\n",
        "print(predict(\"English\", \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Al1qVyPu5f"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.7: Explore the model&rsquo;s predictions </span></strong>\n",
        ">\n",
        "> 1. Play around with these prediction functions for a country or several of which you have robust intuitions about how names from that country might sound. Report on one feature that speaks in favor of the model, and one that suggests that the model is not perfect (or seriously flawed).\n",
        ">  The one flaw of the model is it keeps predicting the same names when the same country or the character sequence is given, suggesting it overfits. The one good feature is that it captures patterns very-well.\n",
        "> 2. Is the prediction function as implemented in this subsection stochastic or not?\n",
        "\n",
        "It is not stochastic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OquONgaSPu5f"
      },
      "source": [
        "Click below to see the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HeEWGHCPu5f"
      },
      "source": [
        "```{toggle}\n",
        "The predict() function itself is not stochastic but rather deterministic, because it uses greedy decoding. This means it will always choose the next most likely character. The variance in the completions comes from the dropout layer in the models architecture leaving space for some randomness. Removing that layer during inference (as described above) will always lead to the same completion given the same input.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY9FtsCaPu5f"
      },
      "source": [
        "## Inverting the generation model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6EKSF0MPu5f"
      },
      "source": [
        "The model we have trained here could be consider a **speaker model**: the model generates language (very limited in scope but still).\n",
        "Additionally, the model can be used to probe how likely a particular name would be (as a generated string) for a particular category/country.\n",
        "So, essentially we get something like $P_{S}(name \\mid category)$ as a speaker likelihood function.\n",
        "For instance, we can do this:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Wge1JK9ePu5f",
        "outputId": "d49c67ec-c9d3-46e0-b5b8-3a9d664a0ba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.77774429321289\n",
            "20.544687271118164\n"
          ]
        }
      ],
      "source": [
        "print(get_surprisal_item(\"German\", \"Franke\"))\n",
        "print(get_surprisal_item(\"Arabic\", \"Franke\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH661UWpPu5f"
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.4.8: Category prediction with Bayes rule </span></strong>\n",
        ">\n",
        "> 1. Use the model as a speaker likelihood function to compute, via Bayes rule, the probability $P(category\\mid name)$ for the names &ldquo;Dovesky&rdquo;, &ldquo;Jackson&rdquo; and &ldquo;Satoshi&rdquo;. Compare the results against those obtained by the RNN-based classifier reported in [this tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) (all the way at the end).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsyxyablPu5g"
      },
      "source": [
        "Click below to see the solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "TjCkM5bXPu5g"
      },
      "outputs": [],
      "source": [
        "# names_data\n",
        "n_names = 0\n",
        "for key in train_data:\n",
        "    n_names += len(train_data[key])\n",
        "n_categories = len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "GxW387EwPu5g",
        "outputId": "7d07ce93-73cc-4df6-d0a3-27fe429f49a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logspace: P(Czech | Doveski) = -2.2774643898010254\n",
            "Logspace: P(German | Doveski) = -2.5991015434265137\n",
            "Logspace: P(Arabic | Doveski) = -3.68674373626709\n",
            "Logspace: P(Japanese | Doveski) = -3.150683879852295\n",
            "Logspace: P(Chinese | Doveski) = -4.266093730926514\n",
            "Logspace: P(Vietnamese | Doveski) = -4.516214370727539\n",
            "Logspace: P(Russian | Doveski) = -2.217602252960205\n",
            "Logspace: P(French | Doveski) = -3.0485265254974365\n",
            "Logspace: P(Irish | Doveski) = -2.9182817935943604\n",
            "Logspace: P(English | Doveski) = -2.630126714706421\n",
            "Logspace: P(Spanish | Doveski) = -3.2798540592193604\n",
            "Logspace: P(Greek | Doveski) = -2.8038394451141357\n",
            "Logspace: P(Italian | Doveski) = -2.6790053844451904\n",
            "Logspace: P(Portuguese | Doveski) = -3.931386947631836\n",
            "Logspace: P(Scottish | Doveski) = -2.7779057025909424\n",
            "Logspace: P(Dutch | Doveski) = -2.5868515968322754\n",
            "Logspace: P(Korean | Doveski) = -5.119194984436035\n",
            "Logspace: P(Polish | Doveski) = -2.1369545459747314\n",
            "1.0000001\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Logspace: P(Czech | Jackson) = -2.823012590408325\n",
            "Logspace: P(German | Jackson) = -2.6330292224884033\n",
            "Logspace: P(Arabic | Jackson) = -3.995298385620117\n",
            "Logspace: P(Japanese | Jackson) = -3.775749683380127\n",
            "Logspace: P(Chinese | Jackson) = -3.8832006454467773\n",
            "Logspace: P(Vietnamese | Jackson) = -5.087978839874268\n",
            "Logspace: P(Russian | Jackson) = -2.8469948768615723\n",
            "Logspace: P(French | Jackson) = -2.681166410446167\n",
            "Logspace: P(Irish | Jackson) = -2.6554155349731445\n",
            "Logspace: P(English | Jackson) = -2.0508365631103516\n",
            "Logspace: P(Spanish | Jackson) = -4.013689041137695\n",
            "Logspace: P(Greek | Jackson) = -3.7807905673980713\n",
            "Logspace: P(Italian | Jackson) = -4.152534484863281\n",
            "Logspace: P(Portuguese | Jackson) = -4.4155802726745605\n",
            "Logspace: P(Scottish | Jackson) = -1.3113625049591064\n",
            "Logspace: P(Dutch | Jackson) = -2.764634847640991\n",
            "Logspace: P(Korean | Jackson) = -4.27211856842041\n",
            "Logspace: P(Polish | Jackson) = -2.8109161853790283\n",
            "1.0\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "Logspace: P(Czech | Satoshi) = -2.558912992477417\n",
            "Logspace: P(German | Satoshi) = -2.9577224254608154\n",
            "Logspace: P(Arabic | Satoshi) = -2.6314480304718018\n",
            "Logspace: P(Japanese | Satoshi) = -1.6356502771377563\n",
            "Logspace: P(Chinese | Satoshi) = -3.7302706241607666\n",
            "Logspace: P(Vietnamese | Satoshi) = -4.701850891113281\n",
            "Logspace: P(Russian | Satoshi) = -2.469816207885742\n",
            "Logspace: P(French | Satoshi) = -3.0107638835906982\n",
            "Logspace: P(Irish | Satoshi) = -3.13441801071167\n",
            "Logspace: P(English | Satoshi) = -2.807342052459717\n",
            "Logspace: P(Spanish | Satoshi) = -3.4486944675445557\n",
            "Logspace: P(Greek | Satoshi) = -2.5304372310638428\n",
            "Logspace: P(Italian | Satoshi) = -2.5062448978424072\n",
            "Logspace: P(Portuguese | Satoshi) = -4.200685977935791\n",
            "Logspace: P(Scottish | Satoshi) = -3.1872949600219727\n",
            "Logspace: P(Dutch | Satoshi) = -3.4587676525115967\n",
            "Logspace: P(Korean | Satoshi) = -4.942294120788574\n",
            "Logspace: P(Polish | Satoshi) = -3.0965688228607178\n",
            "0.9999999\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# function to retrieve conditional probability for (name|country)\n",
        "def get_prob(country, name):\n",
        "    category_tensor_ = category_tensor(country)\n",
        "    input_line_tensor = input_tensor(name)\n",
        "    target_line_tensor = target_tensor(name)\n",
        "    hidden = rnn.init_hidden()\n",
        "    log_prob = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "\n",
        "        output, hidden = rnn(category_tensor_, input_line_tensor[i], hidden, False)\n",
        "        output = output.squeeze()\n",
        "        log_prob += output[target_line_tensor[i]].detach().numpy()\n",
        "    return np.exp(log_prob/input_line_tensor.size(0))\n",
        "\n",
        "\n",
        "# get probability of categories\n",
        "p_categories = {}\n",
        "for key in names_data:\n",
        "    p_categories[key] = len(names_data[key])/n_names\n",
        "\n",
        "# get the conditional probabilities P(name|cat) from model\n",
        "cond_prob_names = {\"Doveski\":[], \"Jackson\":[], \"Satoshi\":[]}\n",
        "country2idx = {}\n",
        "for i, key in enumerate(names_data):\n",
        "    # create mapping from index to country\n",
        "    country2idx[key] = i\n",
        "    for name in cond_prob_names:\n",
        "        prob = get_prob(key, name)\n",
        "        cond_prob_names[name].append(prob)\n",
        "\n",
        "# calculate probability of name as P(name) = sum(P(name|category) * P(category)) for all categories\n",
        "p_names = {\"Doveski\":0, \"Jackson\":0, \"Satoshi\":0}\n",
        "for name in p_names:\n",
        "    for country in names_data:\n",
        "        p_names[name] += cond_prob_names[name][country2idx[country]] * 1/18\n",
        "\n",
        "# apply bayes law\n",
        "for name in cond_prob_names:\n",
        "    total = 0\n",
        "    for country in names_data:\n",
        "        # print(f\"P({country} | {name}) = {(cond_prob_names[name][country2idx[country]] * p_categories[country]) / p_names[name]}\")\n",
        "        print(f\"Logspace: P({country} | {name}) = {np.log((cond_prob_names[name][country2idx[country]] * 1/18) / p_names[name])}\")\n",
        "        total += (cond_prob_names[name][country2idx[country]] * 1/18) / p_names[name]\n",
        "    print(total)\n",
        "    print(\"\\n------------------------------------------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW_hzEyaPu5g"
      },
      "source": [
        "```{toggle}\n",
        "While the numbers don't exactly match with the tutorial, the general tendency is the same except for \"satoshi\", where our model predicts more correctly japanese as the most likely class as opposed to italian in the tutorial.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bQGaogVPu5g"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "npNLG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "org": null,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}