{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBCkuAJJmykc"
      },
      "source": [
        "# Homework 2: Fine-tuning & Prompting of LMs (51 points)\n",
        "\n",
        "The focus of this homework is on one prominent fine-tuning technique -- reinforcement learning from human feedback -- and on critically thinking about prompting techniques and papers about language models\n",
        "\n",
        "### Logistics\n",
        "\n",
        "* submission deadline: June 3rd th 23:59 German time via Moodle\n",
        "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW2.ipynb** containing your solutions of the homework.\n",
        "* please solve and submit the homework **individually**!\n",
        "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yt1W8d6mykg"
      },
      "source": [
        "## Exercise 1: Advanced prompting strategies (16 points)\n",
        "\n",
        "The lecture discussed various sophisticated ways of prompting language models for generating texts. Please answer the following questions about prompting techniques in context of different models, and write down your answers, briefly explaining them (max. 3 sentences). Feel free to actually try out some of the prompting strategies to play around with them and build your intuitions.\n",
        "\n",
        "> Consider the following language models:\n",
        "> * GPT-4, Qwen-2.5-Coder-32B, Mistral-24B-Instruct, Llama-2-70b-base.\n",
        ">  \n",
        "> Consider the following prompting / generation strategies:\n",
        "> * tree-of-thought reasoning, zero-shot chain-of-thought prompting, few-shot prompting, self-reflection prompting.\n",
        ">\n",
        "> For each model:\n",
        "> * which strategies do you think work well, and why?\n",
        ">\n",
        "> For each prompting strategy:\n",
        "> * Name an example task or context, and model, in which you would think they work best. Briefly justify why."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Models**\n",
        "* **GPT-4:** Zero-shot CoT prompting enables complex reasoning capabilities through intermediate reasoning steps. One recent idea essentially involves adding \"Let's think step by step\" to the original prompt. It is very effective for GPT-4 because GPT-4 has strong reasoning capabilities without providing examples inside the prompt, therefore this technique will trigger a reasoning process in GPT-4. Other techniques can be also integrated well into GPT-4 because of its multi-step reasoning , domain-specific context and self-reflection capabilities. However, this [paper](https://arxiv.org/abs/2305.12147#:~:text=To%20bridge%20the%20gap%2C%20this%20paper%20presents%20LogiCoT%2C,instructions%20for%20prompting%20GPT-4%20to%20generate%20chain-of-thought%20rationales.) suggests that it's performance is highly leveraged combined with CoT reasoning with instruction tuning.\n",
        "* **Qwen-2.5-Coder-32B:** This is a task-specific model focused on code generation. Therefore, prompting techniques that is most effective in code generation tasks would be suitable. Since it requires previous examples for coding tasks, few-shot prompting would work well. Also , if model has strong in-context learning capabilities it can leverage zero-shot chain-of-thought prompting (if it was trained on enough reasoning-related coding data). Tree of thought reasoning is also useful since it looks for possible answers with a strategical point of view, and choose the best possible answer. However, self-reflection prompting wouldn't be a very good choice for code generation in my opinion when generating code from scratch. Still, it would be a very good fit for code correction or debugging tasks.\n",
        "* **Mistral-24B-Instruct:** This model become prominent with its reasoning capabilities in terms of instruction following. Among these prompting techniques, tree-of-thought seems suitable for this model. Few-shot is also effective because it is a popular method for instruction following tasks. Self-reflection would be useful because model can revise its earlier errors in the prompt session, improving its instruction following capacities.\n",
        "* **Llama-2-70b-base:** This model has strong text generation capabilities. However, it is a base model so it is not fine-tuned for specific tasks. Thus, few-shot prompting is a suitable choice for this model. Tree-of-thought can be also used but zero-shot CoT wouldn't be an ideal choice since model needs examples based on the specific task.\n",
        "\n",
        "### **Prompting Techniques:**\n",
        "\n",
        "\n",
        "*   **Tree-of-thought reasoning:** It is best for complex tasks that require exploration or strategic lookahead. It encourages exploration over thoughts that serve as intermediate steps for general problem solving such as multi-step mathematical problem or puzzles. (e.g. Game of 24 ). It showed a great improvement for GPT-3.5 and GPT-4 models as suggested in this [paper](https://arxiv.org/abs/2305.10601)\n",
        "*   **Zero-shot chain-of-thought prompting:** As explained above, this prompting technique is particularly preferred for tasks that require multi-step logical reasoning, such as arithmetical problems, common-sense reasoning and logical problems. If the problem can be decomposed into clear reasoning steps, it leverages model's performance. An example prompt: \" I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "Let's think step by step.\" It is useful for models that have strong reasoning patterns without the need of in-context examples during pretraining, such as GPT-4. Also, as this [paper](https://arxiv.org/abs/2305.12147)suggested, LLaMA-7b shows a promising performance improvement if it is fine-tuned for instruction such as LLaMA-7b-logicot , trained on Logicot dataset.\n",
        "*   **Few-shot prompting:** Few-shot prompting can be used as a technique to enable in-context learning where we provide example in the prompt to get better performance. The examples serve as conditioning for subsequent examples where we would like the model to generate a response.It can be used in a task where model needs to correctly use a new word in a sentence. It can also perform very well in text classification, sentiment analysis or content generation, providing only a few examples. Qwen-2.5-Coder could be best for this approach since it generates code and a task-specific model.\n",
        "*   **Self-reflection prompting:** It is designed to help agents improve their performance by reflecting on past mistakes and incorporating that knowledge into future decisions. Thus, making it well-suited for tasks where the agent needs to learn through trial and error, such as decision-making, reasoning, and programming. GPT-4 and Qwen-2.5-Coder would perform very well on these.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I-048r4bu1ee"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1oT2yGYmyki"
      },
      "source": [
        "## Exercise 2: RLHF for summarization (15 points)\n",
        "\n",
        "In this exercise, we want to fine-tune GPT-2 to generate human-like news summaries, following a procedure that is very similar to the example of the movie review generation from [sheet 4.1](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/04a-finetuning-RL.html). The exercise is based on the paper by [Ziegler et al. (2020)](https://arxiv.org/pdf/1909.08593).\n",
        "\n",
        "To this end, we will use the following components:\n",
        "* in order to initialize the policy, we use GPT-2 that was already fine-tuned for summarization, i.e., our SFT model is [this](https://huggingface.co/gavin124/gpt2-finetuned-cnn-summarization-v2)\n",
        "* as our reward model, we will use a task-specific reward signal, namely, the ROUGE score that evaluates a summary generated by a model against a human \"gold standard\" summary.\n",
        "* a dataset of CNN news texts and human-written summaries (for computing the rewards) for the fine-tuning which can be found [here](https://huggingface.co/datasets/abisee/cnn_dailymail). Please note that we will use the *validation* split because we only want to run short fine-tuning.\n",
        "\n",
        "**NOTE:** for building the datset and downloading the pretrained model, ~4GB of space will be used.\n",
        "\n",
        "> **YOUR TASK:**\n",
        ">\n",
        "> Your job for this task is to set up the **GRPO-based** training with the package `trl`, i.e., the set up step 3 of [this](https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg) figure. GRPO (Group Relative Policy Optimization) is an RL algorithm that was proposed by [Shao et al. (2024)](https://arxiv.org/pdf/2402.03300) for the DeepSeek math model.\n",
        "> 1. Please complete the code or insert comments what a particular line of code does below where the comments says \"#### YOUR CODE / COMMENT HERE ####\". For this and for answering the questions, you might need to dig a bit deeper into the working of GRPO, the algorithm that we are using for training. You can find relevant information on the implementation, e.g., [here](https://huggingface.co/docs/trl/main/en/grpo_trainer).\n",
        "> 2. To test your implementation, you can run the training for ~250 steps, but you are NOT required to train the full model since it will take too long. We will NOT be evaluating your submission based on the performance of the model.\n",
        "> 3. Answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "40TMKQMtmykl",
        "outputId": "4f49a3b4-28ae-4fbd-e697-98c087aa94ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gcsfs==2025.3.0\n",
            "  Downloading gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting fsspec==2025.3.0\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting accelerate==1.6.0\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting trl==0.17.0\n",
            "  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2025.3.0) (3.11.15)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2025.3.0) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2025.3.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs==2025.3.0) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs==2025.3.0) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs==2025.3.0) (2.32.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.6.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.6.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.6.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==1.6.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.6.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.6.0) (0.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.6.0) (0.5.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.17.0) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.17.0) (4.52.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2025.3.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2025.3.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2025.3.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2025.3.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2025.3.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2025.3.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2025.3.0) (1.20.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2025.3.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2025.3.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2025.3.0) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.6.0) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.6.0) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2025.3.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2025.3.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2025.3.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2025.3.0) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.6.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.6.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.6.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate==1.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.6.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate==1.6.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl==0.17.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl==0.17.0) (0.21.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs==2025.3.0) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2025.3.0) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2025.3.0) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2025.3.0) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2025.3.0) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.17.0) (2.19.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2025.3.0) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2025.3.0) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2025.3.0) (1.26.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.17.0) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2025.3.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2025.3.0) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.6.0) (3.0.2)\n",
            "Downloading gcsfs-2025.3.0-py2.py3-none-any.whl (36 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ad26a72060be87d519b4dbc4223b75ed6440b58efc1cd8c1cb788d66e8da1e8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, rouge_score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, datasets, evaluate, accelerate, trl, gcsfs\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.7.0\n",
            "    Uninstalling accelerate-1.7.0:\n",
            "      Successfully uninstalled accelerate-1.7.0\n",
            "  Attempting uninstall: gcsfs\n",
            "    Found existing installation: gcsfs 2025.3.2\n",
            "    Uninstalling gcsfs-2025.3.2:\n",
            "      Successfully uninstalled gcsfs-2025.3.2\n",
            "Successfully installed accelerate-1.6.0 datasets-3.6.0 evaluate-0.4.3 fsspec-2025.3.0 gcsfs-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge_score-0.1.2 trl-0.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "accelerate",
                  "datasets"
                ]
              },
              "id": "2c8ad6bd559c49579d968fc999946fc8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install gcsfs==2025.3.0 fsspec==2025.3.0 accelerate==1.6.0 trl==0.17.0 evaluate rouge_score datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_T3mbsq-mykn"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import (\n",
        "    GRPOTrainer,\n",
        "    GRPOConfig,\n",
        ")\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "OOOS76V7myko",
        "outputId": "111c8ffa-31c0-42a9-d816-d1dab36deeac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "GRPOConfig.__init__() got an unexpected keyword argument 'num_training_epochs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4c2ff525d352>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# - Disabling reporting/logging to external services.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m config = GRPOConfig(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m####  Initial learning rate for optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.41e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: GRPOConfig.__init__() got an unexpected keyword argument 'num_training_epochs'"
          ]
        }
      ],
      "source": [
        "# This code defines a configuration object for fine-tuning a language model using GRPO (Generative Reinforcement Pretraining with Offline feedback).\n",
        "# Configuration class for the GRPOTrainer.\n",
        "# It sets essential hyperparameters for the training process, such as:\n",
        "# - The learning rate for the optimizer.\n",
        "# - The total number of steps to train.\n",
        "# - Batch size per device.\n",
        "# - The number of generations (samples) per prompt during training.\n",
        "# - The number of training epochs.\n",
        "# - How often to log metrics during training.\n",
        "# - Disabling reporting/logging to external services.\n",
        "\n",
        "config = GRPOConfig(\n",
        "    ####  Initial learning rate for optimizer.\n",
        "    learning_rate=1.41e-5,\n",
        "    #### The total number of training steps to perform until max_steps is reached. Overrides num_train_epochs.\n",
        "    max_steps=250,\n",
        "    #### The batch size per device accelerator core/CPU for training.\n",
        "    per_device_train_batch_size=8,\n",
        "    #### Number of generations per prompt to sample.\n",
        "    num_generations=8,\n",
        "    #### Set the number of overall training epochs to 1.\n",
        "    num_training_epochs=1,\n",
        "    #### Frequency of tracking metrics such as loss captured in every 1 step.\n",
        "    logging_steps=1,\n",
        "    #### Results or logs are not reported to any integration.\n",
        "    report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpKw8lkmykq"
      },
      "source": [
        "We load the CNN dataset and truncate the texts to 512 tokens, because we don't want the training to be too memory heavy and we want to have \"available\" some tokens for the generation (GPT-2's context window size is 1024). Then we tokenize each text and pad it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_J5igZOmykr"
      },
      "outputs": [],
      "source": [
        "def build_dataset(\n",
        "        model_name,\n",
        "        dataset_name=\"abisee/cnn_dailymail\"\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Build dataset for training. This builds the dataset from `load_dataset`.\n",
        "\n",
        "    Args:\n",
        "        model_name (`str`):\n",
        "            The name of the SFT model to be used, so that the matchin tokenizer can be loaded.\n",
        "        dataset_name (`str`):\n",
        "            The name of the dataset to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dataloader (`torch.utils.data.DataLoader`):\n",
        "            The dataloader for the dataset.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'left'\n",
        "    # load the datasets\n",
        "    ds = load_dataset(dataset_name, '1.0.0', split=\"validation\")\n",
        "\n",
        "    def tokenize(sample):\n",
        "        sample[\"input_ids\"] = tokenizer.encode(\n",
        "            sample[\"article\"],\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        # get the truncated natural language text, too\n",
        "        sample[\"prompt\"] = sample[\"article\"][:512]\n",
        "        # get the ground truth summary\n",
        "        sample[\"ground_truth\"] = sample[\"highlights\"]\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4n-x-N4myks"
      },
      "outputs": [],
      "source": [
        "# use tokenizer from HF named: \"gavin124/gpt2-finetuned-cnn-summarization-v2\"\n",
        "# build the dataset\n",
        "dataset =  build_dataset(\"gavin124/gpt2-finetuned-cnn-summarization-v2\")\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2piqrkQmykt"
      },
      "outputs": [],
      "source": [
        "# inspect a sample of the dataset\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYVcoLsGmykv"
      },
      "source": [
        "We load the tokenizer corresponsing to the SFT GPT2 model that we already used above to pretokenize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF9yKngimykv"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gavin124/gpt2-finetuned-cnn-summarization-v2\")\n",
        "\n",
        "tokenizer.padding_side='left'\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtHndH34mykw"
      },
      "source": [
        "Below, we define our custom reward function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSbtP4nRmykx"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def reward_fn(\n",
        "        output: list[str],\n",
        "        original_summary: list[str]\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Compute a reward signal based on the ROUGE-1 score between the model's outputs and the reference summaries.\n",
        "\n",
        "    Args:\n",
        "        output (list[str]): A list of generated summaries (model outputs).\n",
        "        original_summary (list[str]): A list of reference summaries (ground truth).\n",
        "\n",
        "    Returns:\n",
        "        list[torch.Tensor]: A list of ROUGE-1 scores, where each score corresponds to a pair of generated and reference summaries.\n",
        "                            Each score is returned as a PyTorch tensor.\n",
        "\n",
        "    Notes:\n",
        "        - The function uses the ROUGE-1 metric to evaluate the overlap of unigrams between the predicted and reference texts.\n",
        "        - Scores are computed for each summary pair individually.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for o, s in list(zip(output, original_summary)):\n",
        "      score = rouge.compute(predictions=[o.strip()], references=[s])[\"rouge1\"]\n",
        "      scores.append(torch.tensor(score))\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIJY4xIOmyky"
      },
      "source": [
        "Nest, we set up the trainer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzlyBR9dmykz"
      },
      "outputs": [],
      "source": [
        "grpo_trainer = GRPOTrainer(\n",
        "    # Pass the configuration dictionary we prepared above.\n",
        "    args=config,\n",
        "    # Pass the model name to fine-tune.\n",
        "    model=\"gavin124/gpt2-finetuned-cnn-summarization-v2\",\n",
        "    # Pass the reward function we defined above.\n",
        "    reward_funcs=reward_fn,\n",
        "    # Provide the tokenizer.\n",
        "    processing_class=tokenizer,\n",
        "    # Provide the training dataset for the model to learn from.\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_trainer.train()"
      ],
      "metadata": {
        "id": "us5zomAGOC7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAwuaGTFmykz"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE: plot the loss and the rewards of the model training by accessing the trainer logs under grpo_trainer.state.log_history ####\n",
        "#### YOUR COMMENT HERE: do the plots indicate a trend towards successful training? ####\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the logs from the trainer\n",
        "logs = grpo_trainer.state.log_history\n",
        "\n",
        "# Prepare lists for loss and reward values\n",
        "steps = []\n",
        "losses = []\n",
        "rewards = []\n",
        "\n",
        "for entry in logs:\n",
        "    if 'loss' in entry:\n",
        "        steps.append(entry.get('step', len(steps)))  # Fallback to index if 'step' not present\n",
        "        losses.append(entry['loss'])\n",
        "    if 'reward' in entry:\n",
        "        rewards.append(entry['reward'])\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "axs[0].plot(steps, losses, label='Loss')\n",
        "axs[0].set_xlabel('Step')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Training Loss Over Steps')\n",
        "axs[0].legend()\n",
        "axs[0].grid()\n",
        "\n",
        "# Plot Rewards\n",
        "axs[1].plot(range(len(rewards)), rewards, label='Reward', color='green')\n",
        "axs[1].set_xlabel('Step')\n",
        "axs[1].set_ylabel('Reward')\n",
        "axs[1].set_title('Reward Over Steps')\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wucGSy0gmyk0"
      },
      "source": [
        "> **QUESTIONS:**\n",
        ">\n",
        "> 1. Suppose the plots of rewards below show training metrics for different runs of the summarization model training. Interpret what each of the plots tells us about training success; i.e., did the training run go well on this run? Do we expect to get good summaries? Why? Be concise!\n",
        "> 2. We have truncated the query articles to maximally 512 tokens. Given that we are using ROUGE with respect to ground truth summaries as a reward, why might this be problematic?\n",
        "> 3. GRPO is an algorithm improving over the PPO algorithm (Proximal Policy Optimization). What is they aspect that helps improve over PPO? Explain briefly.\n",
        "> 4. In the GRPO paper referenced above, on page 14, you can find the pseudo-algorithm for GRPO. For lines  1--4, 7--8 of the pseudo-code, write down what in our code above instatiates the concepts in the pseudo-code.\n",
        "> 5. In your own words, explain intuititvely what the role of the *group* in the algorithm is and why it is used. Use max. 3 sentences.\n",
        "> 6.  Name the parameter in the code above that determines the group size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05aiqIDJmyk0"
      },
      "source": [
        "![img](https://github.com/CogSciPrag/Understanding-LLMs-course/blob/main/understanding-llms/homework/data/rewards.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdbGO81Omyk1"
      },
      "source": [
        "## Exercise 3: First neural LM (20 points)\n",
        "\n",
        "Next to reading and understanding package documentations, a key skill for NLP researchers and practitioners is reading and critically assessing NLP literature. The density, but also the style of NLP literature has undergone a significant shift in the recent years with increasing acceleration of progress. Your task in this exercise is to read a paper about one of the first successful neural langauge models, understand its key architectural components and compare how these key components have evolved in modern systems that were discussed in the lecture.\n",
        "\n",
        "> Specifically, please read the paper by [Bengio et al. (2003)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) and answer the following questions:\n",
        ">\n",
        "> * How were words / tokens represented? What is the difference / similarity to modern LLMs?\n",
        "> * How was the context represented? What is the difference / similarity to modern LLMs?\n",
        "> * What is the curse of dimensionality? Give a concrete example in the context of language modeling.\n",
        "> * Which training data was used? What is the difference / similarity to modern LLMs?\n",
        "> * Which components of the Bengio et al. (2003) model (if any) can be found in modern LMs?\n",
        ">\n",
        "\n",
        "Furthermore, your task is to carefully dissect the paper by Bengio et al. (2003) and analyse its structure and style in comparison to another more recent paper:  [Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n",
        "\n",
        "**TASK:**\n",
        "\n",
        "> For each section of the Bengio et al. (2003) paper, what are key differences between the way it is written, the included contents, to the BERT paper (Devlin et al., 2019)? What are key similarities? Write max. 2 sentences per section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZGkNDQQRFUA"
      },
      "source": [
        "# **ANSWERS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> * **How were words / tokens represented? What is the difference/similarity to modern LLMs?**\n",
        ">\n",
        ">  They are represented as a finite set. Each word in the vocabulary are associated with distributed feature vectors, so embeddings. Modern LLMs also use embeddings but not in this way.\n",
        "> * **How was the context represented? What is the difference / similarity to modern LLMs?**\n",
        ">\n",
        ">  Context is represented through mapping an input sequence of feature vectors into a single vector. The size of the sequence is based on the previous n-1 words, in an n-gram model. Modern LLMs don't use fixed window size , they use self-attention mechanism. Therefore they can capture longer dependencies.\n",
        "> * **What is the curse of dimensionality? Give a concrete example in the context of language modeling.**\n",
        ">\n",
        ">  A word sequence on which the model will be tested is likely to be different from all sequences seen during training. If we want to model the joint distribution of 5 consecutive words in a natural language with vocabulary size of 10,000 , there will be potentially 10000^5 -1 = 10^20 -1 free parameters.\n",
        "> * **Which training data was used? What is the difference / similarity to modern LLMs?**\n",
        ">\n",
        ">  Brown corpus consisting of ~1.18M words, with 800K for training, 200K for validation and ~180K for testing.\n",
        "Another experiment is also done on text from AP News from 1995 and 1996 with a training set consisting of around 14M words. The similarity is that the modern LLMs also use corpuses, but they are very large compared to the ones used in this paper. Nowadays LLMs are trained on even billions of tokens-\n",
        "> * **Which components of the Bengio et al. (2003) model (if any) can be found in modern LMs?**\n",
        ">\n",
        ">  Embeddings, auto-regressive modelling, using softmax before output and they also used Cross-entropy loss and Maximum log likelihood."
      ],
      "metadata": {
        "id": "PZgsMa-ndF3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Abstract:\n",
        "Bengio paper was way more difficult for me to grasp because it is abstracter and based on theoritical concepts , and it proposes mainly an abstract concept as a solution to a problem. BERT paper on the other hand, is easier to understand, uses a simple language, even starting from the Abstract. It shows and promotes model's performance results, based on many different experiments, benchmarks, and evaluation metrics. It is strongly empirical.\n",
        "\n",
        "#### Model Architecture:\n",
        "Since Bengio paper is one of the first succesful Natural Language Models, it of course builds upon many mathematically-heavy theorems and proposals. Architecture is detailly explained , starting from the theoricial concept \"curse of dimensionality\" , and explains their solution (embedding) and one by one, context window, linear layers and the whole architecture in detail. It requires good sense of mathematics.In constrast to that, BERT is built upon already existing and largely adopted architecture, therefore it only explains key differences that it has over the other existing Transformer-based models. Thus, it is not mathematically heavy, but requires good knowledge on modern LM architecture.\n",
        "\n",
        "\n",
        "#### Training Objective / Loss\n",
        "Bengio paper includes heavy explanations of the core concepts such as log-likelihood, gradient descent, probability distributions, embeddings.BERT is,on the other hand, simpler since it uses adopted methods. It doesn't explain the mathematical background, rather provides citations, and focuses on Masked LM or NSP and give which loss type they used for this pre-training types.\n",
        "\n",
        "#### Experiments\n",
        "Bengio uses a proof of concept while conducting two experiments on small datasets named Brown Corpus and AP Newsire. It has limited tasks in its experiments. BERT is in contrast, heavily relying on empirical results on a rich variety of experiments such as well-known benchmarks(GLUE, SQuAD) and downstream tasks. It has clear comparisions of quantitive results, also uses vast datasets named BooksCorpus and Wikipedia.\n",
        "\n",
        "#### Results and Discussion\n",
        "Bengio paper doesn't have that extensive quantitive results since it doesn't conduct extensive experiments, rather it uses its theoritical propositions and discusses its limitations.However, BERT, is in a sense promoting the model , focusing on performance gains, task-specific results and improvements, and then point out some limitations such as computing time and hyperparameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "37LZ8oQJjafm"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}